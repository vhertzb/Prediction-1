% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  12pt,
  ignorenonframetext,
]{beamer}
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{part title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{part title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usetheme[]{AnnArbor}
\usecolortheme{dolphin}
\usefonttheme{structurebold}
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Prediction},
  pdfauthor={Vicki Hertzberg},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\newif\ifbibliography
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

\title{Prediction}
\author{Vicki Hertzberg}
\date{31 March 2021}

\begin{document}
\frame{\titlepage}

\begin{frame}
  \tableofcontents[hideallsubsections]
\end{frame}
\hypertarget{prediction}{%
\section{Prediction}\label{prediction}}

\hypertarget{models}{%
\subsection{Models}\label{models}}

\begin{frame}{Generalized Linear Models}
\protect\hypertarget{generalized-linear-models}{}

Many variables have distributions that are consistent with an
\textcolor{orange}{exponential family.}

\bigskip This is the form
\(f(y|\theta) = exp(\eta(\theta)T(y) - A(\theta) + B(y))\).

Examples are

\begin{enumerate}[<+->]
\tightlist
\item
  Normal:
  \(f(y|\mu, \sigma^2) = \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{(y-\mu)^2}{2\sigma^2}}\)
\item
  Bernoulli: \(f(y|\pi) = y^{\pi}(1-y)^{1-\pi}\)
\item
  Exponential: \(f(y|\lambda) = \lambda e^{\lambda y}\) for \(y \ge 0.\)
\end{enumerate}

\end{frame}

\begin{frame}{Generalized Linear Models}
\protect\hypertarget{generalized-linear-models-1}{}

Note that for these distributions we have the following:

\begin{enumerate}[<+->]
\tightlist
\item
  Normal: \(E(Y) = \mu\)
\item
  Bernoulli: \(E(Y) = \pi\)
\item
  Exponential: \(E(Y)=\lambda^{-1}\)
\end{enumerate}

\end{frame}

\begin{frame}{Generalized Linear Model}
\protect\hypertarget{generalized-linear-model}{}

For the exponential family we can say \(E(Y) = A(\theta)\)

Consider the concept of simple linear regression, which we can write as
\(E(Y) = \boldsymbol{X}\beta\)

If we extend the concept of regression to this, we can create a
\textcolor{orange}{generalized linear model}, writing
\(E(Y) = A(\boldsymbol{X}\beta)\).

\end{frame}

\begin{frame}{Generalized Linear Model}
\protect\hypertarget{generalized-linear-model-1}{}

The generalized linear model consists of three parts:

\begin{enumerate}[<+->]
\tightlist
\item
  A \textcolor{orange}{probability function} from the exponential family
\item
  A \textcolor{orange}{linear predictor}, \(\eta = \boldsymbol{X}\beta\)
\item
  A \textcolor{orange}{link function}, \(g(\cdot)\), such that
  \(E(y)=g^{-1}(\boldsymbol{X}\beta)\) (or
  \(g(E(y))=\boldsymbol{X}\beta\))
\end{enumerate}

\end{frame}

\begin{frame}{Generalized Linear Model}
\protect\hypertarget{generalized-linear-model-2}{}

For the simple linear regression case we have:

\begin{enumerate}[<+->]
\tightlist
\item
  The \textcolor{orange}{probability function} is a normal distribution
\item
  The \textcolor{orange}{link function} is
  \(g(E(y))= \boldsymbol{X}\beta\)
\end{enumerate}

\end{frame}

\begin{frame}{Generalized Linear Model}
\protect\hypertarget{generalized-linear-model-3}{}

For \emph{logistic regression} we have:

\begin{enumerate}[<+->]
\tightlist
\item
  The \textcolor{orange}{probability function} is a Bernoulli
  distribution
\item
  The \textcolor{orange}{link function} is
  \(g(E(y))=\frac{e^{\boldsymbol{X}\beta}}{1+e^{\boldsymbol{X}\beta}}\)
\end{enumerate}

\end{frame}

\begin{frame}{Generalized Linear Model}
\protect\hypertarget{generalized-linear-model-4}{}

For \emph{time to event} we have

\begin{enumerate}[<+->]
\tightlist
\item
  The \textcolor{orange}{probability function} is an exponential
  distribution
\item
  The \textcolor{orange}{link function} is
  \(g(E(y))={(\boldsymbol{X}\beta)}^{-1}\)
\end{enumerate}

\end{frame}

\hypertarget{explaining-v-predicting}{%
\subsection{Explaining v Predicting}\label{explaining-v-predicting}}

\begin{frame}{What do we mean by Explaining?}
\protect\hypertarget{what-do-we-mean-by-explaining}{}

We postulate the \(\mathcal{X}\) causes \(\mathcal{Y}\) via a function
\(\mathcal{F}\). We operationalize this as a statistical model,
\(E(y)=f(\boldsymbol{X})\), where \(f\in\mathcal{F}\)

Usually as scientists we want to tease out \emph{causal} relationships.

In an explanatory model, we want to determine how a set of independent
variables, (\(\boldsymbol{X}\)), relates to a dependent outcome, \(Y\).

Goal: to have adequate power and minimal bias in order to match \(f\) to
\(\mathcal{F}\) as closely as possible.

\end{frame}

\begin{frame}{What do we mean by Predicting?}
\protect\hypertarget{what-do-we-mean-by-predicting}{}

\(E(y)=f(\boldsymbol{X})\), \(f\in\mathcal{F}\)

Statisticians are poor at \emph{prediction}, in general.

In a predictive model, we want to determine how well the relationships
between a set of independent variables, (\(\boldsymbol{X}\)), and a
dependent outcome, \(Y\), can be used to predict a new or future
observation.

Goal: to use \(f\) to generate new \(Y\) values.

\end{frame}

\begin{frame}{What difference does it make?}
\protect\hypertarget{what-difference-does-it-make}{}

Explaining:

\begin{enumerate}[<+->]
\tightlist
\item
  \(f\) represents the causal relationship between \(\boldsymbol{X}\)
  and \(Y\)
\item
  \(f\) is estimated to test a causal hypothesis
\item
  Modeling is retrospective, always dealing with data that have been
  collected, and \(f\) is used to test a pre-existing hypothesis or set
  of hypotheses
\end{enumerate}

\end{frame}

\begin{frame}{What difference does it make?}
\protect\hypertarget{what-difference-does-it-make-1}{}

Predicting

\begin{enumerate}[<+->]
\tightlist
\item
  \(f\) captures the \emph{association} between \(\boldsymbol{X}\) and
  \(Y\)
\item
  \(f\) is constructed from the data and direct interpretability of the
  relations between \(\boldsymbol{X}\) and \(Y\) is not necessary or
  required.
\item
  forward-looking, with \(f\) being used to predict a new observation
  \(Y\)
\end{enumerate}

\end{frame}

\begin{frame}{What difference does it make?}
\protect\hypertarget{what-difference-does-it-make-2}{}

Finally, consider the expected predicted error for a new observation at
value \(x\) with a quadratic loss function: \begin{align}
EPE &= E(Y-\hat{f}(x))^2\\
    &= E(Y-f(x))^2 - [E(\hat{f}(x))-f(x)]^2 + E(\hat{f}(x)-E(\hat{f}(x)))^2\\
    &= Var(Y) + Bias^2 + Var(\hat{f}(x))
\end{align}

Bias results when you mis-specify \(f\). Estimation variance (3rd term)
results from using a sample to estimate \(f\). The first term is error
that results even if the model is correctly specified and accurately
estimated.

\end{frame}

\begin{frame}{What difference does it make?}
\protect\hypertarget{what-difference-does-it-make-3}{}

Explaining: focus is on minimizing bias

Prediction: focus is on minimizing the combination of bias and estimate
variance.

Thus sometimes the ``wrong'' model can predict better than the correct
one.

\end{frame}

\hypertarget{metrics}{%
\section{Metrics}\label{metrics}}

\begin{frame}{Metrics for explanatory models}
\protect\hypertarget{metrics-for-explanatory-models}{}

Focus: estimated strength of relationship

\begin{enumerate}[<+->]
\tightlist
\item
  \(R^2\)
\item
  Statistical significance with overall F-type statistics
\end{enumerate}

\end{frame}

\begin{frame}{Metrics for predictive models}
\protect\hypertarget{metrics-for-predictive-models}{}

Focus: predictive accuracy / predictive power (performance of
\(\hat{f}\) on new data)

Validate model with a \emph{new} dataset, typically by performing
cross-validation.

\begin{enumerate}[<+->]
\tightlist
\item
  Divide data into 10 parts.
\item
  Estimate \(f\) from parts 1 - 9, then apply to part 10, how well do
  you do?
\item
  Repeat by estimating from parts 1-8 and 10, then apply to part 9 and
  assess.
\item
  Lather, rinse, and repeat until you have done this 10 times.
\item
  Metrics include area under the Receiver Operating Characteristic curve
  (ROC) (typically called AUC), confusion matrix, calibration,
  discrimination ability, Akaike Information Criterion (AIC)
\end{enumerate}

\end{frame}

\hypertarget{practicalities}{%
\section{Practicalities}\label{practicalities}}

\hypertarget{for-better-clinical-prediction-models}{%
\subsection{For better clinical prediction
models}\label{for-better-clinical-prediction-models}}

\begin{frame}{Seven Steps for Development}
\protect\hypertarget{seven-steps-for-development}{}

Step 1: What is the research question and what do the data look like?

Step 2: Code predictors

Step 3: Model specification

Step 4: Model estimation

Step 5: Evaluate model performance

Step 6: Internal validation

Step 7: Model presentation

\end{frame}

\hypertarget{seven-steps-for-model-development}{%
\subsection{Seven Steps for Model
Development}\label{seven-steps-for-model-development}}

\begin{frame}{Step 1: Problem definition and data inspection}
\protect\hypertarget{step-1-problem-definition-and-data-inspection}{}

What factors predict the endpoint?

What is the risk of the endpoint given a combination of factors?

Endpoint examples:

\begin{enumerate}[<+->]
\tightlist
\item
  Functional rating
\item
  Binary outcome like 30-day mortality, development of a disease
\item
  Time to an event, like death or disease progression
\end{enumerate}

\end{frame}

\begin{frame}{Step 1: Problem definition and data inspection}
\protect\hypertarget{step-1-problem-definition-and-data-inspection-1}{}

What is already known about the predictors?

Need close collaborations between the clinicians and the statisticians.

\end{frame}

\begin{frame}{Step 1: Problem definition and data inspection}
\protect\hypertarget{step-1-problem-definition-and-data-inspection-2}{}

How were patients selected?

\end{frame}

\begin{frame}{Step 1: Problem definition and data inspection}
\protect\hypertarget{step-1-problem-definition-and-data-inspection-3}{}

How to deal with treatment effects?

Include only the placebo group

\end{frame}

\begin{frame}{Step 1: Problem definition and data inspection}
\protect\hypertarget{step-1-problem-definition-and-data-inspection-4}{}

Were the predictors reliably and completely measured?

Missing data

\end{frame}

\begin{frame}{Step 1: Problem definition and data inspection}
\protect\hypertarget{step-1-problem-definition-and-data-inspection-5}{}

Is the endpoint of interest?

Preference for hard endpoints.

\end{frame}

\begin{frame}{Step 2: Coding of predictors}
\protect\hypertarget{step-2-coding-of-predictors}{}

Categorical variables: collapse categories with infrequent occurrence

Continous variables: in inital development, do not categorize
(inefficient).

When you decide to roll-out, consider categorization for purposes of
being user friendly \emph{if} there is not too much of a loss of
information.

\end{frame}

\begin{frame}{Step 3: Model specification}
\protect\hypertarget{step-3-model-specification}{}

How to choose? Stepwise procedures can perform poorly in small datasets.

Additivity association? Choice of interaction terms.

\end{frame}

\begin{frame}{Step 3: Model specification}
\protect\hypertarget{step-3-model-specification-1}{}

Tradeoff between

Simple robust model Finely-tuned model with many independent variables

\end{frame}

\begin{frame}{Step 4: Model estimation}
\protect\hypertarget{step-4-model-estimation}{}

How to estimate those regression coefficients?

When using generalized linear model, use maximum likelihood for
estimation

\end{frame}

\begin{frame}{Step 4: Model estimation}
\protect\hypertarget{step-4-model-estimation-1}{}

Model may be overfitted, and need to ``shrink'' the regression
coefficients towards zero, as they are typically too optimistic.

There are variety of ways to do this. When you are reading the
literature, if someone says they used the

Penalized Maximum Likelihood method, or the Least Absolute Shrinkage and
Selection Operator (LASSO)

just recognize that is what they are doing.

\end{frame}

\begin{frame}{Step 5: Model Performance}
\protect\hypertarget{step-5-model-performance}{}

Assess with some of the measures mentioned previously

\begin{itemize}[<+->]
\tightlist
\item
  ROC curve
\item
  Confusion matrix
\item
  Calibration
\item
  Discrimination ability
\item
  AIC
\end{itemize}

\end{frame}

\begin{frame}{Step 6: Model Validity}
\protect\hypertarget{step-6-model-validity}{}

Internal validity: Validity of claims for the underlying population from
which the data originated (reproducibility)

Use cross validation, bootstrap resampling to determine stability of -
prediction variables - prediction quality

External validity: Generalizability of claims to `plausibly related'
populations

Looking at predictive ability for

\begin{itemize}[<+->]
\tightlist
\item
  future patients (temporal validity)
\item
  patients from another location (geographical validity)
\item
  patients treated in another setting (strong external validation)
\end{itemize}

\end{frame}

\begin{frame}{Step 7: Model presentation}
\protect\hypertarget{step-7-model-presentation}{}

Possibilities include:

\begin{itemize}[<+->]
\tightlist
\item
  Regression formula
\item
  Score charts
\item
  Nomograms
\item
  Apps
\end{itemize}

\end{frame}

\hypertarget{for-better-clinical-prediction-models-1}{%
\subsection{For better clinical prediction
models}\label{for-better-clinical-prediction-models-1}}

\begin{frame}{ABCD's of model validation}
\protect\hypertarget{abcds-of-model-validation}{}

Four key measures in assessing validity of prediction models, related to
calibration, discrimination, and clinical usefulness.

\begin{itemize}[<+->]
\tightlist
\item
  Alpha
\item
  Beta
\item
  Concordance statistic
\item
  Decision Curve Analysis
\end{itemize}

\end{frame}

\hypertarget{abcds-of-model-validation-1}{%
\subsection{ABCD's of Model
Validation}\label{abcds-of-model-validation-1}}

\begin{frame}{Alpha: Calibration in the Large}
\protect\hypertarget{alpha-calibration-in-the-large}{}

Calibration: agreement between observed and predicted, which is assessed
graphically by plotting predicted (x-axis) v observed (y-axis)

Alpha: Compare mean of all predicted to mean of all observed

\end{frame}

\begin{frame}{Beta: Calibration Slope}
\protect\hypertarget{beta-calibration-slope}{}

The calibration slope \(\beta\) is often estimated as smaller than 1 in
a (relatively) small dataset.

Hence the need for cross-validation, bootstrapping estimates, and
external validation

\end{frame}

\begin{frame}{Concordance statistic: Discrimination}
\protect\hypertarget{concordance-statistic-discrimination}{}

Discrimination: ability of the model to distinguish between a patient
with an endpoint and one without the endpoint.

Quantification of this ability is via the concordance statistic, \(c\).

The ROC plots sensitivity (true-positive rate) v 1-specificity
(true-negative rate) for consecutive cutoffs for predicted risk. The
concordance statistic, \(c\), is measured by the AUC.

\end{frame}

\begin{frame}{Decision curve analysis}
\protect\hypertarget{decision-curve-analysis}{}

Decision curve balances those with high risk to those with low risk.

Once a cutpoint has been chosen, summary measures include

\begin{itemize}[<+->]
\tightlist
\item
  Sum of sensitivity and 1-specificity (naive)
\item
  Netbenefit: weighted sum of true- minus false-positives.
\end{itemize}

\end{frame}

\hypertarget{prediction-1}{%
\section{Prediction}\label{prediction-1}}

\hypertarget{summary}{%
\subsection{Summary}\label{summary}}

\begin{frame}{Moral of the Story}
\protect\hypertarget{moral-of-the-story}{}

Models are ``easy'' to develop (``just'' involve computer)

Validation (especially external validation) is much more difficult, but
must be done.

\end{frame}

\end{document}
